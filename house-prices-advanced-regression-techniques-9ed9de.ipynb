{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"House Prices Advanced Regression Techniques\n----------\n\n## Summary\nYou never knew you wanted to go to Ames, Iowa but by the end you will know more than you ever thought possible from a data set compiled by Dean De Cock. There are many cities that have open data sets on housing, like my local town of Burlington Vermont, but in this Kaggle competition we are looking to explore the possible relationship of the sale price to all the other features of a house. \n\nMany but not all of the features of a house share a linear relationship with the Sale Price. By filling missing values, some feature engineering and feature selection determining the Sales Price should not be that far off. In this case I tried applying Random Forest Regression, Lasso and Ridge Regressions to arrive at my final answer \n\n## Goal\nUse the dataset after normalization, filling in missing values and adding new features with a linear regression model to predict sales prices.\n\n## Method\nThere are a large number of features in this data set  and in order to create a good prediction there will have to be a fair amount of work to do in advance. Combining the train and test set will means changes only need to be done once across both data sets. We will be exploring the following steps:\n* Correlation\n* Feature Exploration\n* Missing Values\n* Data Normilization\n* Feature Engineering\n* Assembling dataset\n* Prediction\n\nThere should be several good indicators for sale price, the challenge will be in normalization of the data and feature engineering.","metadata":{"_cell_guid":"b59df2dc-c353-c01c-3da4-60c477b86a5e"}},{"cell_type":"code","source":"# Handle table-like data and matrices\nimport numpy as np\nimport pandas as pd\nimport math\n\n# Modelling Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, LassoLarsCV,Ridge\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n\n# Modelling Helpers\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import Normalizer , scale, OneHotEncoder\nfrom sklearn.model_selection import train_test_split , StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\n\n# Visualisation\nimport matplotlib \nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n# Configure visualisations\n%matplotlib inline","metadata":{"_cell_guid":"2d34f629-19d9-c16e-da5b-18766091db5d","execution":{"iopub.status.busy":"2023-03-18T21:42:13.301371Z","iopub.execute_input":"2023-03-18T21:42:13.302003Z","iopub.status.idle":"2023-03-18T21:42:13.316623Z","shell.execute_reply.started":"2023-03-18T21:42:13.301913Z","shell.execute_reply":"2023-03-18T21:42:13.315335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindf = pd.read_csv(\"../input/train.csv\")\ntestdf = pd.read_csv(\"../input/test.csv\")","metadata":{"_cell_guid":"2d11ce1e-9162-11ad-c6ff-0062cb47b27e","execution":{"iopub.status.busy":"2023-03-18T21:42:13.318755Z","iopub.execute_input":"2023-03-18T21:42:13.319654Z","iopub.status.idle":"2023-03-18T21:42:13.371785Z","shell.execute_reply.started":"2023-03-18T21:42:13.319602Z","shell.execute_reply":"2023-03-18T21:42:13.370889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a single dataframe of both the training and testing data\nwholedf = pd.concat([traindf,testdf])","metadata":{"_cell_guid":"bd3173b0-f4f5-16ad-6bb5-703a021debe9","execution":{"iopub.status.busy":"2023-03-18T21:42:13.373709Z","iopub.execute_input":"2023-03-18T21:42:13.374576Z","iopub.status.idle":"2023-03-18T21:42:13.405553Z","shell.execute_reply.started":"2023-03-18T21:42:13.374528Z","shell.execute_reply":"2023-03-18T21:42:13.404371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wholedf.info()","metadata":{"_cell_guid":"79976670-f0fa-95ec-bf26-ef1516d922ee","execution":{"iopub.status.busy":"2023-03-18T21:42:13.408926Z","iopub.execute_input":"2023-03-18T21:42:13.409389Z","iopub.status.idle":"2023-03-18T21:42:13.466032Z","shell.execute_reply.started":"2023-03-18T21:42:13.409340Z","shell.execute_reply":"2023-03-18T21:42:13.464547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wholedf.head(5)","metadata":{"_cell_guid":"30de2da9-8a62-5a80-cdc5-44eb1c211bfb","execution":{"iopub.status.busy":"2023-03-18T21:42:13.467792Z","iopub.execute_input":"2023-03-18T21:42:13.468236Z","iopub.status.idle":"2023-03-18T21:42:13.519232Z","shell.execute_reply.started":"2023-03-18T21:42:13.468190Z","shell.execute_reply":"2023-03-18T21:42:13.518370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Data\n-----------\nThe data has a wide range of integer, float and categorical information. In the end those will end up as numeric, but not quite yet. There are a couple of groupings of different types of measurements and understanding their differences will help in determining what new features can be created.\n\nNeighborhood — Information about the neighborhood, zoning and lot.\nExamples: MSSubClass, LandContour, Neighborhood, BldgType\n\nDates — Time based data about when it was built, remodeled or sold.\nExample: YearBuilt, YearRemodAdd, GarageYrBlt, YrSold\n\nQuality/Condition — There are categorical assessment of the various features of the houses, most likely from the property assessor.\nExample: PoolQC, SaleCondition,GarageQual, HeatingQC\n\nProperty Features — Categorical collection of additional features and attributes of the building\nExample: Foundation, Exterior1st, BsmtFinType1,Utilities\n\nSquare Footage — Area measurement of section of the building and features like porches and lot area(which is in acres)\nExample: TotalBsmtSF, GrLivArea, GarageArea, PoolArea, LotArea\n\nRoom/Feature Count — Quantitative counts of features (versus categorical) like rooms, prime candidate for feature engineering\nExample: FullBath, BedroomAbvGr, Fireplaces,GarageCars\n\nPricing — Monetary values, one of which is the sales price we are trying to determine\nExamples: SalePrice, MiscVal","metadata":{"_cell_guid":"c51c0dca-b195-76c8-8c27-ab828bd8106a"}},{"cell_type":"code","source":"sns.lmplot(x=\"GrLivArea\", y=\"SalePrice\", data=wholedf);\nplt.title(\"Linear Regression of Above Grade Square Feet and Sale Price\")\nplt.ylim(0,)\nplt.show()","metadata":{"_cell_guid":"ae5a41e9-72ba-a3da-063e-12890b58a792","execution":{"iopub.status.busy":"2023-03-18T21:42:13.520517Z","iopub.execute_input":"2023-03-18T21:42:13.521435Z","iopub.status.idle":"2023-03-18T21:42:14.108657Z","shell.execute_reply.started":"2023-03-18T21:42:13.521398Z","shell.execute_reply":"2023-03-18T21:42:14.107658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x=\"1stFlrSF\", y=\"SalePrice\", data=wholedf);\nplt.title(\"Linear Regression of First Floor Square Feet and Sale Price\")\nplt.ylim(0,)\nplt.show()","metadata":{"_cell_guid":"a27e04a3-1794-54bb-b024-41b2c1eb5be7","execution":{"iopub.status.busy":"2023-03-18T21:42:14.110520Z","iopub.execute_input":"2023-03-18T21:42:14.110923Z","iopub.status.idle":"2023-03-18T21:42:14.633112Z","shell.execute_reply.started":"2023-03-18T21:42:14.110884Z","shell.execute_reply":"2023-03-18T21:42:14.631785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation\n--------------\nA quick correlation check is the best way to the heart of the data set.  There is a far amount of correlation for sales price with a couple of variables:\n\n* OverallQual - 0.790982\n* GrLivArea -  0.708624\n* GarageCars -  0.640409\n* GarageArea -  0.623431\n* TotalBsmtSF -  0.613581\n* 1stFlrSF -  0.605852\n* FullBath -  0.560664\n* TotRmsAbvGrd -  0.533723\n* YearBuilt  - 0.522897\n* YearRemodAdd -  0.507101","metadata":{"_cell_guid":"d0fa7ab1-eead-ceac-f479-dac96a76d051"}},{"cell_type":"code","source":"traincorr = traindf.corr()['SalePrice']\n# convert series to dataframe so it can be sorted\ntraincorr = pd.DataFrame(traincorr)\n# correct column label from SalePrice to correlation\ntraincorr.columns = [\"Correlation\"]\n# sort correlation\ntraincorr2 = traincorr.sort_values(by=['Correlation'], ascending=False)\ntraincorr2.head(15)","metadata":{"_cell_guid":"1e613413-1bcf-11a2-58ea-a3f123646955","execution":{"iopub.status.busy":"2023-03-18T21:42:14.635166Z","iopub.execute_input":"2023-03-18T21:42:14.635611Z","iopub.status.idle":"2023-03-18T21:42:14.660628Z","shell.execute_reply.started":"2023-03-18T21:42:14.635565Z","shell.execute_reply":"2023-03-18T21:42:14.659140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = wholedf.corr()\nsns.heatmap(corr)\nplt.show()","metadata":{"_cell_guid":"5ec42ca1-c8a1-f059-80e9-9308562b64ca","execution":{"iopub.status.busy":"2023-03-18T21:42:14.662404Z","iopub.execute_input":"2023-03-18T21:42:14.663241Z","iopub.status.idle":"2023-03-18T21:42:15.179970Z","shell.execute_reply.started":"2023-03-18T21:42:14.663193Z","shell.execute_reply":"2023-03-18T21:42:15.178769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing Values\n--------------------\nThere is a wide selection of missing values. First it make sense to hit the low hanging fruit first and deal with those that are missing a value or two, then work through what is left. Some of these look to be missing values not because they don’t have data but rather because the building was missing that feature, like a garage. Using Pandas Get.Dummies will sort that problem out into true/false values","metadata":{"_cell_guid":"02842225-8a7b-9356-9f58-b60920ed2263"}},{"cell_type":"code","source":"countmissing = wholedf.isnull().sum().sort_values(ascending=False)\npercentmissing = (wholedf.isnull().sum()/wholedf.isnull().count()).sort_values(ascending=False)\nwholena = pd.concat([countmissing,percentmissing], axis=1)\nwholena.head(36)","metadata":{"_cell_guid":"8fa6fba0-8b5d-09d6-470b-db8345b1de03","execution":{"iopub.status.busy":"2023-03-18T21:42:15.185646Z","iopub.execute_input":"2023-03-18T21:42:15.186859Z","iopub.status.idle":"2023-03-18T21:42:15.227887Z","shell.execute_reply.started":"2023-03-18T21:42:15.186763Z","shell.execute_reply":"2023-03-18T21:42:15.226812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replacing Missing Data\n------------\nFor the categorical information that is missing a single values a quick check shows which ones are dominant and manually replace the missing values. For most it is a quick process, I commented out the Python as I had in order to keep this shorter but feel free to uncomment and take a look.","metadata":{"_cell_guid":"28b883d3-7be2-d667-d35c-60363998771e"}},{"cell_type":"code","source":"#wholedf[[\"Utilities\", \"Id\"]].groupby(['Utilities'], as_index=False).count()\nwholedf['Utilities'] = wholedf['Utilities'].fillna(\"AllPub\")\n\n# wholedf[[\"Electrical\", \"Id\"]].groupby(['Electrical'], as_index=False).count()\nwholedf['Electrical'] = wholedf['Electrical'].fillna(\"SBrkr\")\n\n# wholedf[[\"Exterior1st\", \"Id\"]].groupby(['Exterior1st'], as_index=False).count()\nwholedf['Exterior1st'] = wholedf['Exterior1st'].fillna(\"VinylSd\")\n\n#wholedf[[\"Exterior2nd\", \"Id\"]].groupby(['Exterior2nd'], as_index=False).count()\nwholedf['Exterior2nd'] = wholedf['Exterior2nd'].fillna(\"VinylSd\")","metadata":{"_cell_guid":"14ded018-d126-6cf9-3c17-e186615c2174","execution":{"iopub.status.busy":"2023-03-18T21:42:15.228986Z","iopub.execute_input":"2023-03-18T21:42:15.229933Z","iopub.status.idle":"2023-03-18T21:42:15.239430Z","shell.execute_reply.started":"2023-03-18T21:42:15.229896Z","shell.execute_reply":"2023-03-18T21:42:15.238013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing interger values replace with the median in order to return an integer\nwholedf['BsmtFullBath']= wholedf.BsmtFullBath.fillna(wholedf.BsmtFullBath.median())\nwholedf['BsmtHalfBath']= wholedf.BsmtHalfBath.fillna(wholedf.BsmtHalfBath.median())\nwholedf['GarageCars']= wholedf.GarageCars.fillna(wholedf.GarageCars.median())\n\n# Missing float values were replaced with the mean for accuracy \nwholedf['BsmtUnfSF']= wholedf.BsmtUnfSF.fillna(wholedf.BsmtUnfSF.mean())\nwholedf['BsmtFinSF2']= wholedf.BsmtFinSF2.fillna(wholedf.BsmtFinSF2.mean())\nwholedf['BsmtFinSF1']= wholedf.BsmtFinSF1.fillna(wholedf.BsmtFinSF1.mean())\nwholedf['GarageArea']= wholedf.GarageArea.fillna(wholedf.GarageArea.mean())\nwholedf['MasVnrArea']= wholedf.MasVnrArea.fillna(wholedf.MasVnrArea.mean())","metadata":{"_cell_guid":"6a5417d7-47fc-3027-2613-b70aa2208cd0","execution":{"iopub.status.busy":"2023-03-18T21:42:15.240962Z","iopub.execute_input":"2023-03-18T21:42:15.242082Z","iopub.status.idle":"2023-03-18T21:42:15.291451Z","shell.execute_reply.started":"2023-03-18T21:42:15.242029Z","shell.execute_reply":"2023-03-18T21:42:15.289516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infer Missing Values\nSome of the missing values can be inferred from other values for that given property. The GarageYearBuilt would have to be at the earliest the year the house was built. Likewise TotalBasementSQFeet would have to be equal to the first floor square footage.","metadata":{"_cell_guid":"d92a7940-d4c6-942b-9de8-7c32d9b7c3e3"}},{"cell_type":"code","source":"wholedf.GarageYrBlt.fillna(wholedf.YearBuilt, inplace=True)\nwholedf.TotalBsmtSF.fillna(wholedf['1stFlrSF'], inplace=True)\n\nsns.lmplot(x=\"TotalBsmtSF\", y=\"1stFlrSF\", data=wholedf)\nplt.title(\"Linear Regression of Basement SF and 1rst Floor SQ \")\nplt.xlim(0,)\nplt.ylim(0,)\nplt.show()","metadata":{"_cell_guid":"01d1afea-7010-859c-5695-2b5228f731a3","execution":{"iopub.status.busy":"2023-03-18T21:42:15.293460Z","iopub.execute_input":"2023-03-18T21:42:15.293932Z","iopub.status.idle":"2023-03-18T21:42:15.911890Z","shell.execute_reply.started":"2023-03-18T21:42:15.293885Z","shell.execute_reply":"2023-03-18T21:42:15.910622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lot Frontage\n\nLot Frontage is a bit trickier. There are 486 missing values (17% of total values) but a quick check of correlation shows that there are surprisingly few features that have high correlation outside of Lot Area.","metadata":{"_cell_guid":"c594980f-a87c-bfbe-c071-4c726814c269"}},{"cell_type":"code","source":"lot = wholedf[['LotArea','LotConfig','LotFrontage','LotShape']]\nlot = pd.get_dummies(lot)\nlot.corr()['LotFrontage']","metadata":{"_cell_guid":"09befb87-50cc-efce-b48b-55180eb4c212","execution":{"iopub.status.busy":"2023-03-18T21:42:15.913063Z","iopub.execute_input":"2023-03-18T21:42:15.913411Z","iopub.status.idle":"2023-03-18T21:42:15.936718Z","shell.execute_reply.started":"2023-03-18T21:42:15.913369Z","shell.execute_reply":"2023-03-18T21:42:15.935394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logic dictates that the Lot Area should have a linear relation to Lot Frontage. A quick check with a linear regression of the relationship between Lot Frontage and the square root of Lot Area (or effectively one side) shows we are in the ballpark.","metadata":{"_cell_guid":"1fa53e14-0852-c00e-b636-3d9e8e012796"}},{"cell_type":"code","source":"lot[\"LotAreaUnSq\"] = np.sqrt(lot['LotArea'])","metadata":{"_cell_guid":"60f40eeb-0e2a-5df6-ed79-e70e4667ca84","execution":{"iopub.status.busy":"2023-03-18T21:42:15.938434Z","iopub.execute_input":"2023-03-18T21:42:15.938929Z","iopub.status.idle":"2023-03-18T21:42:15.946988Z","shell.execute_reply.started":"2023-03-18T21:42:15.938881Z","shell.execute_reply":"2023-03-18T21:42:15.945516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.regplot(x=\"LotAreaUnSq\", y=\"LotFrontage\", data=lot);\nplt.xlim(0,)\nplt.ylim(0,)\nplt.title(\"Lot Area to Lot Frontage\")\nplt.show()","metadata":{"_cell_guid":"28e2b520-14db-4249-bb3b-e3739ce616d4","execution":{"iopub.status.busy":"2023-03-18T21:42:15.949020Z","iopub.execute_input":"2023-03-18T21:42:15.949382Z","iopub.status.idle":"2023-03-18T21:42:16.397514Z","shell.execute_reply.started":"2023-03-18T21:42:15.949349Z","shell.execute_reply":"2023-03-18T21:42:16.396194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove all lotfrontage is missing values\nlot = lot[lot['LotFrontage'].notnull()]\n# See the not null values of LotFrontage\nlot.describe()['LotFrontage']","metadata":{"_cell_guid":"16bb6027-716b-c6c2-fdb8-369f002a6ce1","execution":{"iopub.status.busy":"2023-03-18T21:42:16.399123Z","iopub.execute_input":"2023-03-18T21:42:16.399496Z","iopub.status.idle":"2023-03-18T21:42:16.442295Z","shell.execute_reply.started":"2023-03-18T21:42:16.399459Z","shell.execute_reply":"2023-03-18T21:42:16.440933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wholedf['LotFrontage']= wholedf.LotFrontage.fillna(np.sqrt(wholedf.LotArea))\nwholedf['LotFrontage']= wholedf['LotFrontage'].astype(int)","metadata":{"_cell_guid":"c8bff8ba-d224-1cb3-31c4-9c3d0dffbca0","execution":{"iopub.status.busy":"2023-03-18T21:42:16.444177Z","iopub.execute_input":"2023-03-18T21:42:16.444681Z","iopub.status.idle":"2023-03-18T21:42:16.453070Z","shell.execute_reply.started":"2023-03-18T21:42:16.444634Z","shell.execute_reply":"2023-03-18T21:42:16.451650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the missing values are filled in, it is good to confirm that the distribution did not go way out of wack. The blue is the original distribution, the green is the new one with missing values inferred and the red is the curve of the square root of lot area. I looks like a majority of these properties unsurprisingly do not have perfectly square lots but for our purposes, everything looks good.","metadata":{"_cell_guid":"a772171d-3208-bc02-2761-a38f4b7035f0"}},{"cell_type":"code","source":"# Distribution of values after replacement of missing frontage\nsns.kdeplot(wholedf['LotFrontage']);\nsns.kdeplot(lot['LotFrontage']);\nsns.kdeplot(lot['LotAreaUnSq']);\nplt.title(\"Distribution of Lot Frontage\")\nplt.show()","metadata":{"_cell_guid":"43003ab1-f936-26ce-aa7b-db0f5caa8e74","execution":{"iopub.status.busy":"2023-03-18T21:42:16.455400Z","iopub.execute_input":"2023-03-18T21:42:16.455874Z","iopub.status.idle":"2023-03-18T21:42:16.761222Z","shell.execute_reply.started":"2023-03-18T21:42:16.455824Z","shell.execute_reply":"2023-03-18T21:42:16.759860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"countmissing = wholedf.isnull().sum().sort_values(ascending=False)\npercentmissing = (wholedf.isnull().sum()/wholedf.isnull().count()).sort_values(ascending=False)\nwholena = pd.concat([countmissing,percentmissing], axis=1)\nwholena.head(3)","metadata":{"_cell_guid":"0745beac-fda8-45f9-ce80-7a90d46af148","execution":{"iopub.status.busy":"2023-03-18T21:42:16.762492Z","iopub.execute_input":"2023-03-18T21:42:16.762823Z","iopub.status.idle":"2023-03-18T21:42:16.806556Z","shell.execute_reply.started":"2023-03-18T21:42:16.762792Z","shell.execute_reply":"2023-03-18T21:42:16.805367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feaure Engineering\n-----------------------\nNow it is time to create some new features and see how they can help the model’s accuracy. First are a couple of macro creations, like adding all the internal and external square footage together to get the total living space including both floors, garage and external spaces.","metadata":{"_cell_guid":"b60561b3-7712-6cc5-5792-9d5f83888e17"}},{"cell_type":"code","source":"Livingtotalsq = wholedf['TotalBsmtSF'] + wholedf['1stFlrSF'] + wholedf['2ndFlrSF'] + wholedf['GarageArea'] + wholedf['WoodDeckSF'] + wholedf['OpenPorchSF']\nwholedf['LivingTotalSF'] = Livingtotalsq\n\n# Total Living Area divided by LotArea\nwholedf['PercentSQtoLot'] = wholedf['LivingTotalSF'] / wholedf['LotArea']\n\n# Total count of all bathrooms including full and half through the entire building\nwholedf['TotalBaths'] = wholedf['BsmtFullBath'] + wholedf['BsmtHalfBath'] + wholedf['HalfBath'] + wholedf['FullBath']\n\n# Percentage of total rooms are bedrooms\nwholedf['PercentBedrmtoRooms'] = wholedf['BedroomAbvGr'] / wholedf['TotRmsAbvGrd']\n\n# Number of years since last remodel, if there never was one it would be since it was built\nwholedf['YearSinceRemodel'] = 2016 - ((wholedf['YearRemodAdd'] - wholedf['YearBuilt']) + wholedf['YearBuilt'])","metadata":{"_cell_guid":"7bed7d7c-92bd-11fc-cb91-87481649ded2","execution":{"iopub.status.busy":"2023-03-18T21:42:16.808468Z","iopub.execute_input":"2023-03-18T21:42:16.809227Z","iopub.status.idle":"2023-03-18T21:42:16.824072Z","shell.execute_reply.started":"2023-03-18T21:42:16.809177Z","shell.execute_reply":"2023-03-18T21:42:16.822922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Total Square Footage\nThere is a minimal increase of the Condition Rating to square foot, which is not a surprise. However when a linear regression with the Sale Price is taken instead a much more obvious pattern emerges.","metadata":{"_cell_guid":"83cc0da8-0ff6-4152-b78c-a4fa49804c9d"}},{"cell_type":"code","source":"sns.barplot(x=\"OverallCond\", y=\"LivingTotalSF\", data=wholedf)\nplt.title(\"Total Square Footage by Overall Condition Rating\")\nplt.show()\n\nsns.lmplot(x=\"LivingTotalSF\", y=\"SalePrice\", data=wholedf)\nplt.title(\"Relation of Sale Price to Total Square Footage\")\nplt.xlim(0,)\nplt.ylim(0,)\nplt.show()","metadata":{"_cell_guid":"42a27185-36e8-5d2e-7341-7eefd374f0f9","execution":{"iopub.status.busy":"2023-03-18T21:42:16.826169Z","iopub.execute_input":"2023-03-18T21:42:16.826729Z","iopub.status.idle":"2023-03-18T21:42:17.848485Z","shell.execute_reply.started":"2023-03-18T21:42:16.826683Z","shell.execute_reply":"2023-03-18T21:42:17.847153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Total Rooms\nConversely if rooms were what interested you, there is a pronounced relationship between rooms and the number of bathrooms. This is the mean of bathrooms and the line is the variance (how much from the average most values deviate).","metadata":{"_cell_guid":"a7eef1ae-1c1f-908e-4fc2-7ec1e94c4704"}},{"cell_type":"code","source":"ax = sns.barplot(x=\"TotRmsAbvGrd\", y=\"TotalBaths\",data=wholedf)\nplt.title(\"Total Rooms Versus Total Bathrooms\")\nplt.show()","metadata":{"_cell_guid":"7fa8efaf-9e98-cc8b-9db0-7d078bc44067","execution":{"iopub.status.busy":"2023-03-18T21:42:17.849819Z","iopub.execute_input":"2023-03-18T21:42:17.850150Z","iopub.status.idle":"2023-03-18T21:42:18.532676Z","shell.execute_reply.started":"2023-03-18T21:42:17.850118Z","shell.execute_reply":"2023-03-18T21:42:18.531535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sale Month\nOnce last colorful graph before wrapping this up. I have wondered this about the Vermont housing market as the weather can have a large impact on many activities, including looking at houses I would imagine.","metadata":{"_cell_guid":"b6abbdf2-5c7c-bf90-97b6-678228ff6d06"}},{"cell_type":"code","source":"sns.swarmplot(x=\"MoSold\", y=\"SalePrice\", data=wholedf)\nplt.title(\"Sale Price by Month\")\nplt.show()\n\nsns.kdeplot(wholedf['MoSold']);\nplt.title(\"Distribution of Month Sold\")\nplt.xlim(1,12)\nplt.show()","metadata":{"_cell_guid":"a99f975c-01ea-6f1f-c5a4-96223f4f424f","execution":{"iopub.status.busy":"2023-03-18T21:42:18.533906Z","iopub.execute_input":"2023-03-18T21:42:18.534220Z","iopub.status.idle":"2023-03-18T21:42:26.916481Z","shell.execute_reply.started":"2023-03-18T21:42:18.534189Z","shell.execute_reply":"2023-03-18T21:42:26.915031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neighborhoods\nAll of us know that neighborhoods have a wide variety of characteristics. In some ways there is a relationship to sale price, but it is harder to define and less useful in prediction.","metadata":{"_cell_guid":"3e023714-3dc9-b961-4ff2-ac266eda81a4"}},{"cell_type":"code","source":"plt.figure(figsize = (12, 6))\nsns.boxplot(x = 'Neighborhood', y = 'SalePrice',  data = wholedf)\nplt.xticks(rotation=45)\nplt.show()","metadata":{"_cell_guid":"4b13a7c7-48d2-8e28-02b6-3ecf2818f4c6","execution":{"iopub.status.busy":"2023-03-18T21:42:26.917844Z","iopub.execute_input":"2023-03-18T21:42:26.918155Z","iopub.status.idle":"2023-03-18T21:42:27.519194Z","shell.execute_reply.started":"2023-03-18T21:42:26.918123Z","shell.execute_reply":"2023-03-18T21:42:27.517887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features\nThis is a way to edit what feeatures are included in the final model.  I played a bit with which to include, and I have a feeling I may come back to this to re-engineer it.  ","metadata":{"_cell_guid":"154a43fe-3e58-8cd2-d74c-36369c75a204"}},{"cell_type":"code","source":"pricing1 = wholedf[['Id','SalePrice','MiscVal']]\n\nneigh = wholedf[['Neighborhood','MSZoning','MSSubClass','BldgType','HouseStyle']]\n\ndates = wholedf[['YearBuilt','YearRemodAdd','GarageYrBlt','YearSinceRemodel']]\n\nquacon = wholedf[['ExterQual','BsmtQual','PoolQC','Condition1','Condition2','SaleCondition',\n                  'BsmtCond','ExterCond','GarageCond','KitchenQual','GarageQual','HeatingQC','OverallQual','OverallCond']]\n\nfeatures =  wholedf[['Foundation','RoofStyle','RoofMatl','Exterior1st','Exterior2nd',\n                     'MiscFeature','PavedDrive','Utilities',\n                     'Heating','CentralAir','Electrical','Fence']]\n\nsqfoot = wholedf[['LivingTotalSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea',\n                  'GarageArea','WoodDeckSF','OpenPorchSF','LotArea','PercentSQtoLot','LowQualFinSF']]\n\nroomfeatcount = wholedf[['PercentBedrmtoRooms','TotalBaths','FullBath','HalfBath',\n                         'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','GarageType','EnclosedPorch']]\n\n# Splits out sale price for the training set and only has not null values\npricing = wholedf['SalePrice']\npricing = pricing[pricing.notnull()]\n\n# Bringing it all together\nwholedf = pd.concat([pricing1,neigh,dates,quacon,features,sqfoot,roomfeatcount], axis=1)","metadata":{"_cell_guid":"cdc45ed7-c2dc-01f1-3a47-f5bef774f5ce","execution":{"iopub.status.busy":"2023-03-18T21:42:27.520835Z","iopub.execute_input":"2023-03-18T21:42:27.521171Z","iopub.status.idle":"2023-03-18T21:42:27.540470Z","shell.execute_reply.started":"2023-03-18T21:42:27.521137Z","shell.execute_reply":"2023-03-18T21:42:27.539264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Conversion\nIn order for the model to understand categories, first replace all the categorical data with boolean values through Pandas get_dummies.  ","metadata":{"_cell_guid":"4166522d-944f-b5ee-aeca-aff8ef4da6bc"}},{"cell_type":"code","source":"wholedf = pd.get_dummies(wholedf)","metadata":{"_cell_guid":"919f27e4-e6a9-2451-2996-22eac93ab151","execution":{"iopub.status.busy":"2023-03-18T21:42:27.541802Z","iopub.execute_input":"2023-03-18T21:42:27.542656Z","iopub.status.idle":"2023-03-18T21:42:27.585663Z","shell.execute_reply.started":"2023-03-18T21:42:27.542621Z","shell.execute_reply":"2023-03-18T21:42:27.584281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traincorr = wholedf.corr()['SalePrice']\n# convert series to dataframe so it can be sorted\ntraincorr = pd.DataFrame(traincorr)\n# correct column label from SalePrice to correlation\ntraincorr.columns = [\"Correlation\"]\n# sort correlation\ntraincorr2 = traincorr.sort_values(by=['Correlation'], ascending=False)\ntraincorr2.head(15)","metadata":{"_cell_guid":"155c7d9d-1934-3e74-5ec8-28183b38d5cc","execution":{"iopub.status.busy":"2023-03-18T21:42:27.590538Z","iopub.execute_input":"2023-03-18T21:42:27.590905Z","iopub.status.idle":"2023-03-18T21:42:27.977952Z","shell.execute_reply.started":"2023-03-18T21:42:27.590871Z","shell.execute_reply":"2023-03-18T21:42:27.976543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split Database\n--------\nTime to split the database back into two parts, one with sales price and one without","metadata":{"_cell_guid":"67f0745a-4b35-b9d5-3641-ff78970e7a15"}},{"cell_type":"code","source":"train_X = wholedf[wholedf['SalePrice'].notnull()]\ndel train_X['SalePrice']\ntest_X =  wholedf[wholedf['SalePrice'].isnull()]\ndel test_X['SalePrice']","metadata":{"_cell_guid":"7763f182-5012-af95-4b9c-32f6678f281e","execution":{"iopub.status.busy":"2023-03-18T21:42:27.980313Z","iopub.execute_input":"2023-03-18T21:42:27.980665Z","iopub.status.idle":"2023-03-18T21:42:27.992500Z","shell.execute_reply.started":"2023-03-18T21:42:27.980629Z","shell.execute_reply":"2023-03-18T21:42:27.991123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training/Test Dataset\n\nCreate training set assembly","metadata":{"_cell_guid":"946925de-d038-48d8-b09d-1d0227fdbb71"}},{"cell_type":"code","source":"# Create all datasets that are necessary to train, validate and test models\ntrain_valid_X = train_X\ntrain_valid_y = pricing\ntest_X = test_X\ntrain_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )","metadata":{"_cell_guid":"03926d49-c9f1-32dd-91fb-df5df6dd0e70","execution":{"iopub.status.busy":"2023-03-18T21:42:27.994579Z","iopub.execute_input":"2023-03-18T21:42:27.995097Z","iopub.status.idle":"2023-03-18T21:42:28.005465Z","shell.execute_reply.started":"2023-03-18T21:42:27.995042Z","shell.execute_reply":"2023-03-18T21:42:28.003918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\nHere are a variety of models you can try, many performed extremely poorly, Lasso was the best but I plan to go back to this and do further work on refining my features, like log regressions and normalizations.  ","metadata":{"_cell_guid":"bceca0b8-58a7-313f-a0a5-71b86ec89565"}},{"cell_type":"code","source":"# model = RandomForestRegressor()\nmodel = Ridge()\n# model = LassoLarsCV()\n\n# Models that performed substantially worse\n# model = LinearSVC()\n# model = KNeighborsClassifier(n_neighbors = 3)\n# model = GaussianNB()\n# model = LogisticRegression()\n# model = SVC()","metadata":{"_cell_guid":"5b3ec615-4ffd-5cb0-6132-7e9097640459","execution":{"iopub.status.busy":"2023-03-18T21:42:56.668402Z","iopub.execute_input":"2023-03-18T21:42:56.669340Z","iopub.status.idle":"2023-03-18T21:42:56.675003Z","shell.execute_reply.started":"2023-03-18T21:42:56.669287Z","shell.execute_reply":"2023-03-18T21:42:56.673914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's add another algorithm ","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor(learning_rate=0.01,\n                       n_estimators=8000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T21:48:23.721408Z","iopub.execute_input":"2023-03-18T21:48:23.721900Z","iopub.status.idle":"2023-03-18T21:48:23.729598Z","shell.execute_reply.started":"2023-03-18T21:48:23.721857Z","shell.execute_reply":"2023-03-18T21:48:23.728002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.fit( train_X , train_y )\n# Print the Training Set Accuracy and the Test Set Accuracy in order to understand overfitting\nprint (xgb.score( train_X , train_y ) , xgb.score( valid_X , valid_y ))","metadata":{"execution":{"iopub.status.busy":"2023-03-18T21:53:23.707431Z","iopub.execute_input":"2023-03-18T21:53:23.707842Z","iopub.status.idle":"2023-03-18T21:54:15.586500Z","shell.execute_reply.started":"2023-03-18T21:53:23.707807Z","shell.execute_reply":"2023-03-18T21:54:15.585237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id = test_X.Id\nresult = xgb.predict(test_X)\n\n# output = pd.DataFrame( { 'id': id , 'SalePrice': result}, columns=['id', 'SalePrice'] )\npred = pd.DataFrame( { 'id': id , 'SalePrice': result} )\npred = pred[['id', 'SalePrice']]\npred","metadata":{"execution":{"iopub.status.busy":"2023-03-18T22:05:51.628912Z","iopub.execute_input":"2023-03-18T22:05:51.629355Z","iopub.status.idle":"2023-03-18T22:05:51.752031Z","shell.execute_reply.started":"2023-03-18T22:05:51.629314Z","shell.execute_reply":"2023-03-18T22:05:51.750883Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"        id      SalePrice\n0     1461  132058.968750\n1     1462  162751.734375\n2     1463  186750.468750\n3     1464  193834.062500\n4     1465  170356.968750\n...    ...            ...\n1454  2915   92739.015625\n1455  2916   86359.296875\n1456  2917  165414.062500\n1457  2918  112849.226562\n1458  2919  215301.671875\n\n[1459 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1461</td>\n      <td>132058.968750</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1462</td>\n      <td>162751.734375</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1463</td>\n      <td>186750.468750</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1464</td>\n      <td>193834.062500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1465</td>\n      <td>170356.968750</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1454</th>\n      <td>2915</td>\n      <td>92739.015625</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>2916</td>\n      <td>86359.296875</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>2917</td>\n      <td>165414.062500</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>2918</td>\n      <td>112849.226562</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>2919</td>\n      <td>215301.671875</td>\n    </tr>\n  </tbody>\n</table>\n<p>1459 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pred.to_csv(\"solution.csv\", index = False)\npred.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-03-18T22:03:34.255358Z","iopub.execute_input":"2023-03-18T22:03:34.255831Z","iopub.status.idle":"2023-03-18T22:03:34.278566Z","shell.execute_reply.started":"2023-03-18T22:03:34.255781Z","shell.execute_reply":"2023-03-18T22:03:34.277372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit/Accurancy","metadata":{"_cell_guid":"789c0d9d-6f4a-e775-6b43-7fd558378393"}},{"cell_type":"code","source":"model.fit( train_X , train_y )\n\n# Print the Training Set Accuracy and the Test Set Accuracy in order to understand overfitting\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))","metadata":{"_cell_guid":"0fbedad8-157a-a95b-c609-923377a9bb96","execution":{"iopub.status.busy":"2023-03-18T21:52:26.375330Z","iopub.execute_input":"2023-03-18T21:52:26.376089Z","iopub.status.idle":"2023-03-18T21:52:26.426729Z","shell.execute_reply.started":"2023-03-18T21:52:26.376045Z","shell.execute_reply":"2023-03-18T21:52:26.425167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id = test_X.Id\nresult = model.predict(test_X)\n\n# output = pd.DataFrame( { 'id': id , 'SalePrice': result}, columns=['id', 'SalePrice'] )\noutput = pd.DataFrame( { 'id': id , 'SalePrice': result} )\noutput = output[['id', 'SalePrice']]\n\n#output.to_csv(\"solution.csv\", index = False)\noutput.head(10)","metadata":{"_cell_guid":"8555f20d-73ae-cfbc-b2bc-097566613e98","execution":{"iopub.status.busy":"2023-03-18T22:03:50.631282Z","iopub.execute_input":"2023-03-18T22:03:50.632719Z","iopub.status.idle":"2023-03-18T22:03:50.663929Z","shell.execute_reply.started":"2023-03-18T22:03:50.632671Z","shell.execute_reply":"2023-03-18T22:03:50.662483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nThere were a couple of models tried. Random Forest Regression, LassoLarsCV and Ridge were all options and after experimenting I ended up settling on Ridge as the better option. What was fascinating and frustrating was the wide variation in accuracy each time I ran it. The models all seem to have some fluctuation in them. In the end I had about a 82%–90% accuracy. In trying to apply this same process to Burlington’s housing data I ran into even stranger inconsistency and huge over fitting problems.  I have a feeling this will be a work in progress as I try it on other data sets.  \n\nThere were some great kernels out there that helped me work through the problem, definately check the following out:\nhttps://www.kaggle.com/neviadomski/house-prices-advanced-regression-techniques/how-to-get-to-top-25-with-simple-model-sklearn\nhttps://www.kaggle.com/poonaml/house-prices-advanced-regression-techniques/house-prices-data-exploration-and-visualisation\nhttps://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models\nhttps://www.kaggle.com/xchmiao/house-prices-advanced-regression-techniques/detailed-data-exploration-in-python\n","metadata":{"_cell_guid":"f62c79e6-0346-a39b-b09e-433bbdb506b1"}}]}